{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d1a1b3-ea49-412a-bfc7-6b462109f4ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select\n",
    "  *\n",
    "from\n",
    "  information_schema.columns\n",
    "where\n",
    "  table_schema = \"bronze\"\n",
    "order by\n",
    "  table_name,\n",
    "  ordinal_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ebf25e-7b7b-44d7-b075-9b780dba98f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Generate a mapping file template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5741a6e7-caeb-4ef0-822e-1e74888504ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_mappings_template():\n",
    "    query = \"\"\"\n",
    "    select\n",
    "        *\n",
    "    from\n",
    "        information_schema.columns\n",
    "    where\n",
    "        table_schema = 'bronze'\n",
    "    order by\n",
    "        table_name,\n",
    "        ordinal_position\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.sql(query).toPandas()\n",
    "\n",
    "    yaml_content = \"\"\"# mappings:\n",
    "    #   - target: silver.table_name.column_name\n",
    "    #     source:\n",
    "    #       - bronze.table_name.column_name\n",
    "    mappings:\n",
    "    - ignored:\n",
    "    \"\"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        yaml_content += f\"      - bronze.{row['table_name']}.{row['column_name']}\\n\"\n",
    "\n",
    "    return yaml_content\n",
    "\n",
    "\n",
    "print(generate_mappings_template())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae7bb815-7b5b-4779-a667-4e430a9e3e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "126e5bc8-5c30-4c31-a589-18bb7a8aa7fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from collections import defaultdict\n",
    "\n",
    "mappings_yaml_file_path = \"/Volumes/rk_workspace/bronze/disk/mappings.yaml\"\n",
    "\n",
    "with open(mappings_yaml_file_path, \"r\") as file:\n",
    "    mappings_content = file.readlines()\n",
    "mappings = yaml.safe_load(\"\".join(mappings_content))\n",
    "\n",
    "flat_mappings = []\n",
    "table_mappings = defaultdict(set)\n",
    "\n",
    "\n",
    "def parse_name(name):\n",
    "    parts = name.split(\".\")\n",
    "    return (parts[0], parts[1], parts[2])\n",
    "\n",
    "\n",
    "for mapping in mappings[\"mappings\"]:\n",
    "    if \"ignored\" in mapping:\n",
    "        continue\n",
    "\n",
    "    target_schema, target_table, target_column = parse_name(mapping[\"target\"])\n",
    "\n",
    "    for source in mapping[\"source\"]:\n",
    "        source_schema, source_table, source_column = parse_name(source)\n",
    "        flat_mappings.append(\n",
    "            {\n",
    "                \"source_schema\": source_schema,\n",
    "                \"source_table\": source_table,\n",
    "                \"source_column\": source_column,\n",
    "                \"target_schema\": target_schema,\n",
    "                \"target_table\": target_table,\n",
    "                \"target_column\": target_column,\n",
    "            }\n",
    "        )\n",
    "        table_mappings[f\"{target_schema}.{target_table}\"].add(\n",
    "            f\"{source_schema}.{source_table}\"\n",
    "        )\n",
    "\n",
    "# print(flat_mappings)\n",
    "\n",
    "for target_table in table_mappings:\n",
    "    print(f\"Table: {target_table}\")\n",
    "    target_columns = set()\n",
    "\n",
    "    for mapping in flat_mappings:\n",
    "        if f'{mapping[\"target_schema\"]}.{mapping[\"target_table\"]}' == target_table:\n",
    "            target_columns.add(mapping[\"target_column\"])\n",
    "\n",
    "    print(target_columns)\n",
    "\n",
    "    sql = f\"CREATE OR REPLACE VIEW {target_table} AS (\\n\"\n",
    "\n",
    "    union_all = \"\\n  UNION ALL\\n\\n\"\n",
    "\n",
    "    for source_table in table_mappings[target_table]:\n",
    "\n",
    "        columns = \"\"\n",
    "\n",
    "        for target_column in target_columns:\n",
    "            column_found = False\n",
    "            for mapping in flat_mappings:\n",
    "                if (\n",
    "                    f'{mapping[\"source_schema\"]}.{mapping[\"source_table\"]}'\n",
    "                    == source_table\n",
    "                    and mapping[\"target_column\"] == target_column):\n",
    "                        columns += f\"\\n    {mapping['source_column']} AS {target_column}, \"\n",
    "                        column_found = True\n",
    "                        break\n",
    "            if not column_found:\n",
    "                columns += f\"\\n    NULL AS {target_column}, \"\n",
    "            \n",
    "\n",
    "        sql += f\"  SELECT{columns[:-2]}\\n  FROM\\n    {source_table}\\n{union_all}\"\n",
    "\n",
    "    sql = sql.rstrip(union_all)\n",
    "    sql += \"\\n);\"\n",
    "\n",
    "    print(sql)\n",
    "    spark.sql(sql)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6273847313263691,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. property mapping",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
